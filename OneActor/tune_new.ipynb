{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3378dc64-db66-41cd-b489-5abffd4d63a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/oa_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/oa_venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/workspace/oa_venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import yaml\n",
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa801405-2ae3-4436-b766-c60f6d238681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc886d0-3a38-4900-965d-b4d4b8e0ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/oa_venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/workspace/oa_venv/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler\n",
    "from unet_2d_condition import UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878fa254-02bb-4227-9572-1b72e33efb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "from projector import Projector\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd1ac454-e0bb-4d7a-bb50-7c4a99b65c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "PIL_INTERPOLATION = {\n",
    "        \"linear\": PIL.Image.Resampling.BILINEAR,\n",
    "        \"bilinear\": PIL.Image.Resampling.BILINEAR,\n",
    "        \"bicubic\": PIL.Image.Resampling.BICUBIC,\n",
    "        \"lanczos\": PIL.Image.Resampling.LANCZOS,\n",
    "        \"nearest\": PIL.Image.Resampling.NEAREST,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb78a08-34a9-499e-8b51-108693d915b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__)\n",
    "\n",
    "\n",
    "def init_wanddb(config=None):\n",
    "    wandb.init(project=\"OneActor\", entity=\"eadishcheva\")\n",
    "    run = wandb.init(\n",
    "        # Set the wandb entity where your project will be logged (generally your team name).\n",
    "        entity=\"eadishcheva\",\n",
    "        # Set the wandb project where this run will be logged.\n",
    "        project=\"OneActor\",\n",
    "        # Track hyperparameters and run metadata.\n",
    "        config=config,\n",
    "    )\n",
    "    return run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd11eea8-0449-4882-a86a-6412f0899c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "292782e6-b22c-4e66-a921-f1e659b89df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_model_class_from_model_name_or_path(\n",
    "    pretrained_model_name_or_path: str, revision: str = None, subfolder: str = \"text_encoder\"\n",
    "):\n",
    "    text_encoder_config = PretrainedConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=subfolder, revision=revision\n",
    "    )\n",
    "    model_class = text_encoder_config.architectures[0]\n",
    "\n",
    "    if model_class == \"CLIPTextModel\":\n",
    "        from transformers import CLIPTextModel\n",
    "\n",
    "        return CLIPTextModel\n",
    "    elif model_class == \"CLIPTextModelWithProjection\":\n",
    "        from transformers import CLIPTextModelWithProjection\n",
    "\n",
    "        return CLIPTextModelWithProjection\n",
    "    else:\n",
    "        raise ValueError(f\"{model_class} is not supported.\")\n",
    "    \n",
    "def save_progress(projector, accelerator, save_path):\n",
    "    logger.info(\"Saving embeddings\")\n",
    "    learned_projector = (\n",
    "        accelerator.unwrap_model(projector)\n",
    "    )\n",
    "\n",
    "    torch.save(learned_projector, save_path)\n",
    "\n",
    "\n",
    "human_templates = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a portrait of a {}\",\n",
    "    \"a cropped photo of the {}\",\n",
    "    \"the photo of a {}\",\n",
    "    \"a photo of a beautiful {}\",\n",
    "    \"a realistic photo of a {}\",\n",
    "    \"a dark photo of the {}\",\n",
    "    \"a character photo of a {}\",\n",
    "    \"a photo of the cool {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a face photo of the {}\",\n",
    "    \"a cropped face of a {}\",\n",
    "    \"a photo of the {}\",\n",
    "    \"a good photo of the {}\",\n",
    "    \"a high-quality photo of one {}\",\n",
    "    \"a close-up photo of the {}\",\n",
    "    \"a rendition of the {}\",\n",
    "    \"a photo of the clean {}\",\n",
    "    \"a rendition of a {}\",\n",
    "    \"a photo of a nice {}\",\n",
    "    \"a good photo of a {}\",\n",
    "    \"an image of a {}\",\n",
    "    \"a snapshot of a {}\",\n",
    "    \"a person's photo of a {}\",\n",
    "    \"an individual's photo of a {}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5abd55d1-3d48-4c0d-be4c-6a8860ef0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# input: latent_sequence(con&uncon), prompt_embed, prompt, base_prompt\n",
    "# input: latent_sequence(con&uncon), prompt_embed, prompt, base_prompt\n",
    "class OneActorDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_dir,\n",
    "        use_mask=False,\n",
    "        repeats=200,\n",
    "        interpolation=\"bicubic\",\n",
    "        flip_p=0.5,\n",
    "        set=\"train\",\n",
    "        config=None,\n",
    "    ):\n",
    "        self.target_root = target_dir\n",
    "        self.size = 1024\n",
    "        self.latent_size = 128\n",
    "\n",
    "        self.base_condition = config['base']\n",
    "        self.flip_p = flip_p\n",
    "        self.neg_num = 5\n",
    "\n",
    "        self.use_mask = use_mask\n",
    "        \n",
    "        self._length = repeats\n",
    "        \n",
    "        self.target_image_paths = self.target_root + \"/target.jpg\"\n",
    "        self.base_image_paths = [\n",
    "            os.path.join(f\"{self.target_root}/base\", file_path) for file_path in os.listdir(f\"{self.target_root}/base\") \\\n",
    "                if os.path.splitext(file_path)[1] == '.jpg']\n",
    "\n",
    "        self.num_base = len(self.base_image_paths)\n",
    "\n",
    "        self._length = 200\n",
    "        self.interpolation = {\n",
    "            \"linear\": PIL_INTERPOLATION[\"linear\"],\n",
    "            \"bilinear\": PIL_INTERPOLATION[\"bilinear\"],\n",
    "            \"bicubic\": PIL_INTERPOLATION[\"bicubic\"],\n",
    "            \"lanczos\": PIL_INTERPOLATION[\"lanczos\"],\n",
    "        }[interpolation]\n",
    "        \n",
    "        self.templates = human_templates\n",
    "\n",
    "        with open(self.target_root+f'/target_data.pkl', 'rb') as f:\n",
    "            target_data = pickle.load(f)\n",
    "\n",
    "        with open(self.target_root+f'/base/base_data_list.pkl', 'rb') as f:\n",
    "            base_data = pickle.load(f)\n",
    "\n",
    "        self.target_data = target_data\n",
    "        self.base_data = base_data\n",
    "        \n",
    "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)    # randomly flip images\n",
    "\n",
    "        self.h_mid = self.target_data['h_mid']\n",
    "        self.prompt_embed = self.target_data['prompt_embed']\n",
    "        self.base_mid = [h['h_mid'][-1] for h in self.base_data]\n",
    "\n",
    "        if self.use_mask:\n",
    "            self.target_mask = self.target_data['mask_64']\n",
    "            self.target_mask = self.target_mask.unsqueeze(0).unsqueeze(0)  # => (1, 1, H, W)\n",
    "            self.target_mask = F.interpolate(self.target_mask, (self.latent_size, self.latent_size), mode='bilinear', align_corners=True)\n",
    "            self.target_mask = self.target_mask.squeeze(0)\n",
    "            self.base_masks = [torch.ones(size=(1, self.latent_size, self.latent_size))] * self.num_base\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        example = {}\n",
    "\n",
    "        imgs = []\n",
    "        h_mid_list = []\n",
    "        \n",
    "        if self.use_mask:\n",
    "            mask_tensors = []\n",
    "        # target samples\n",
    "\n",
    "        imgs.append(self.target_image_paths)\n",
    "        h_mid_list.append(self.h_mid[-1])\n",
    "        if self.use_mask:\n",
    "            mask_tensors.append(self.target_mask)\n",
    "        \n",
    "        # base samples\n",
    "        for i in range(self.neg_num):\n",
    "            ind = random.randint(0, len(self.base_image_paths)-1)\n",
    "            imgs.append(self.base_image_paths[ind])\n",
    "            h_mid_list.append(self.base_mid[ind])\n",
    "            if self.use_mask:\n",
    "                mask_tensors.append(self.base_masks[ind])\n",
    "        \n",
    "        img_tensors = []\n",
    "        text_list = []\n",
    "            \n",
    "        for n, img_path in enumerate(imgs):\n",
    "            image = Image.open(img_path)\n",
    "            if not image.mode == \"RGB\":\n",
    "                image = image.convert(\"RGB\")\n",
    "            # default to score-sde preprocessing\n",
    "            image = np.array(image).astype(np.uint8)\n",
    "            image = Image.fromarray(image)\n",
    "            image = image.resize((self.size, self.size), resample=self.interpolation)\n",
    "            image_f = self.flip_transform(image)\n",
    "            flip_ind = image_f != image\n",
    "            image = image_f\n",
    "            \n",
    "            image = np.array(image).astype(np.uint8)\n",
    "            image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "\n",
    "            img_tensors.append(torch.from_numpy(image).permute(2, 0, 1))\n",
    "\n",
    "            text = random.choice(self.templates).format(self.base_condition)\n",
    "            text_list.append(text)\n",
    "\n",
    "            if self.use_mask:\n",
    "                if flip_ind:\n",
    "                    mask_tensors[n] = torch.flip(mask_tensors[n], [2])\n",
    "                mask_tensors[n] =  mask_tensors[n].repeat(4, 1, 1)\n",
    "        if self.use_mask:\n",
    "            mask_tensors.append(mask_tensors[0])\n",
    "\n",
    "        img_tensors.append(img_tensors[0])\n",
    "        text_list.append(text_list[0])\n",
    "        h_mid_list.append(random.choice(h_mid_list))\n",
    "\n",
    "        example[\"pixel_values\"] = torch.stack(img_tensors)\n",
    "        if self.use_mask:\n",
    "            example[\"mask_pixel_values\"] = torch.stack(mask_tensors)\n",
    "        example['text'] = text_list\n",
    "        example['base'] = self.base_condition\n",
    "        example['h_mid'] = torch.stack(h_mid_list)\n",
    "    \n",
    "        return example    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fae9940-b856-44b6-b0c2-a57e6f4f32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # get environment configs\n",
    "    with open(\"PATH.json\",\"r\") as f:\n",
    "        ENV_CONFIGS = json.load(f)\n",
    "    # get user configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "459b7d59-7c8e-4e79-ae45-daa422a1f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(\"./config/gen_cs_adventure.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    device = config['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a247a195-fa6c-4001-8bf0-1c29fd44fb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_id = 2504292003\n"
     ]
    }
   ],
   "source": [
    "    target_dir = './experiments/consistory_adventurer'\n",
    "    for _, tgt_dirs, _ in os.walk(target_dir):\n",
    "        break\n",
    "    \n",
    "    exp_id = '2504292003'\n",
    "\n",
    "    print(f\"exp_id = {exp_id}\")\n",
    "\n",
    "\n",
    "    if exp_id not in tgt_dirs:\n",
    "        print(\"Base image is not generated\")\n",
    "\n",
    "    target_dir += f\"/{exp_id}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db2b722-b7c4-4e09-986a-248cec5ae3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/oa_venv/lib/python3.10/site-packages/accelerate/accelerator.py:401: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "04/29/2025 20:17:35 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "    accelerator_project_config = ProjectConfiguration(project_dir=target_dir, logging_dir='logs')\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=1,\n",
    "        mixed_precision=\"fp16\",\n",
    "        log_with=\"tensorboard\",\n",
    "        project_config=accelerator_project_config,\n",
    "    )\n",
    "    # Make one log on every process with the configuration for debugging.\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(accelerator.state, main_process_only=False)\n",
    "    if accelerator.is_local_main_process:\n",
    "        transformers.utils.logging.set_verbosity_warning()\n",
    "        diffusers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "        diffusers.utils.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cfded40-e522-41f2-9b82-22ca1f13f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b051793-a850-45d2-b6f7-f20fc4b721f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mask = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b298010-c667-42ba-a755-4f7471802fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    now = datetime.now()\n",
    "\n",
    "    train_id = now.strftime(\"%y%m%d%H%M\") # uuid.uuid4()\n",
    "    train_id = str(train_id)\n",
    "    output_dir = f\"{target_dir}/output_{train_id}\"\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(f\"{output_dir}/ckpt\", exist_ok=True)\n",
    "        os.makedirs(f\"{output_dir}/weight\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b235a31-7513-425c-8628-3afcf88d7e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "/workspace/oa_venv/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "{'dynamic_thresholding_ratio', 'clip_sample_range', 'thresholding', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n"
     ]
    }
   ],
   "source": [
    "    pretrained_model_name_or_path = ENV_CONFIGS['paths']['sdxl_path']\n",
    "    # Load the tokenizers\n",
    "    tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"tokenizer\",\n",
    "        revision=None,\n",
    "        use_fast=False,\n",
    "    )\n",
    "    tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"tokenizer_2\",\n",
    "        revision=None,\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "    # import correct text encoder classes\n",
    "    text_encoder_cls_one = import_model_class_from_model_name_or_path(\n",
    "        pretrained_model_name_or_path, None\n",
    "    )\n",
    "    text_encoder_cls_two = import_model_class_from_model_name_or_path(\n",
    "        pretrained_model_name_or_path, None, subfolder=\"text_encoder_2\"\n",
    "    )\n",
    "\n",
    "    # Load scheduler and models\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    # Check for terminal SNR in combination with SNR Gamma\n",
    "    text_encoder_one = text_encoder_cls_one.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"text_encoder\",\n",
    "    )\n",
    "    text_encoder_two = text_encoder_cls_two.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"text_encoder_2\",\n",
    "    )\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"vae\",\n",
    "    )\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"unet\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36ccd97a-2bbd-4cdc-88c9-56dace02fda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Projector(\n",
       "  (net): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1280, 640, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "      (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Residual(\n",
       "        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Residual(\n",
       "        (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv3): Conv2d(640, 320, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Residual(\n",
       "        (conv1): Conv2d(320, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv3): Conv2d(320, 160, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Flatten()\n",
       "    (5): Linear(in_features=2560, out_features=2048, bias=True)\n",
       "    (6): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    vae.requires_grad_(False)\n",
    "    text_encoder_one.requires_grad_(False)\n",
    "    text_encoder_two.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "\n",
    "    # Build projector\n",
    "    projector = Projector(1280, 2048)\n",
    "    # Fire projector\n",
    "    projector.requires_grad_(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4348ee3-b6a9-468c-8b64-4c91bcc86123",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b598328-dd02-46e4-80cb-b5e15fa75087",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5a777b-d3e5-4c0c-8d52-174d2545057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "        projector.parameters(),  # only optimize the embeddings\n",
    "        lr=config['lr'],\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=1e-2,\n",
    "        eps=1e-8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0175ee63-e0e0-426e-a808-5f85665d1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OneActorDataset(\n",
    "        target_dir=target_dir,\n",
    "        use_mask=use_mask,\n",
    "        config=config,\n",
    "        set='train',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0f7232c-2461-403c-9e37-0fe49eee116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b7538a1-d48a-4b9a-a7a8-75613ea4afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader))\n",
    "\n",
    "max_train_steps = config['epochs'] * num_update_steps_per_epoch\n",
    "overrode_max_train_steps = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fd0f65b-a80f-48af-9bca-414947618705",
   "metadata": {},
   "outputs": [],
   "source": [
    "    lr_scheduler = get_scheduler(\n",
    "        \"constant\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=500 * accelerator.num_processes,\n",
    "        num_training_steps=max_train_steps * accelerator.num_processes,\n",
    "        num_cycles=1,\n",
    "    )\n",
    "\n",
    "    projector.train()\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    projector, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        projector, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    # For mixed precision training we cast all non-trainable weigths (vae, non-lora text_encoder and non-lora unet) to half-precision\n",
    "    # as these weights are only used for inference, keeping weights in full precision is not required.\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    # Move vae and unet to device and cast to weight_dtype\n",
    "\n",
    "    with torch.cuda.amp.autocast(dtype=weight_dtype):\n",
    "        unet.to(accelerator.device, dtype=weight_dtype)\n",
    "        text_encoder_one.to(accelerator.device, dtype=weight_dtype)\n",
    "        text_encoder_two.to(accelerator.device, dtype=weight_dtype)\n",
    "        vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    # with torch.cuda.amp.autocast(dtype=torch.float32):\n",
    "    #     vae.to(accelerator.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61ad5da4-1204-45db-9e26-4a23e134706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader))\n",
    "    if overrode_max_train_steps:\n",
    "        max_train_steps = config['epochs'] * num_update_steps_per_epoch\n",
    "    # Afterwards we recalculate our number of training epochs\n",
    "    config['epochs'] = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # We need to initialize the trackers we use, and also store our configuration.\n",
    "    # The trackers initializes automatically on the main process.\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"OneActor\", config=config)\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = config['batch_size'] * accelerator.num_processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ba226aa-5591-4a4b-b006-9abeac4870e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meadishcheva\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Diploma/OneActor/wandb/run-20250429_201759-j9cueeuk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eadishcheva/OneActor/runs/j9cueeuk' target=\"_blank\">dazzling-firebrand-9</a></strong> to <a href='https://wandb.ai/eadishcheva/OneActor' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eadishcheva/OneActor' target=\"_blank\">https://wandb.ai/eadishcheva/OneActor</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eadishcheva/OneActor/runs/j9cueeuk' target=\"_blank\">https://wandb.ai/eadishcheva/OneActor/runs/j9cueeuk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-firebrand-9</strong> at: <a href='https://wandb.ai/eadishcheva/OneActor/runs/j9cueeuk' target=\"_blank\">https://wandb.ai/eadishcheva/OneActor/runs/j9cueeuk</a><br> View project at: <a href='https://wandb.ai/eadishcheva/OneActor' target=\"_blank\">https://wandb.ai/eadishcheva/OneActor</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250429_201759-j9cueeuk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Diploma/OneActor/wandb/run-20250429_201800-y2dbtabw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eadishcheva/OneActor/runs/y2dbtabw' target=\"_blank\">wobbly-dawn-10</a></strong> to <a href='https://wandb.ai/eadishcheva/OneActor' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eadishcheva/OneActor' target=\"_blank\">https://wandb.ai/eadishcheva/OneActor</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eadishcheva/OneActor/runs/y2dbtabw' target=\"_blank\">https://wandb.ai/eadishcheva/OneActor/runs/y2dbtabw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    with open(f\"{output_dir}/log_train.log\", 'w') as log_file:\n",
    "        print(f\"num_examples: {len(train_dataset)}\", file=log_file)\n",
    "        print(f\"num_epochs: {config['epochs']}\", file=log_file)\n",
    "        print(f\"batch_size: {config['batch_size']}\", file=log_file)\n",
    "        print(f\"total_batch_size: {total_batch_size}\", file=log_file)\n",
    "        print(f\"max_train_steps: {max_train_steps}\", file=log_file)\n",
    "        print(f\"target_dir: '{target_dir}'\", file=log_file)\n",
    "        print(f\"train_id: '{train_id}'\", file=log_file)\n",
    "        print(f\"use_mask: '{use_mask}'\", file=log_file)\n",
    "        # print(f\"mask_power: '{mask_power}'\", file=log_file)\n",
    "        # print(f\"mask_alpha: '{mask_alpha}'\", file=log_file)\n",
    "\n",
    "    run = init_wanddb(config={\n",
    "        \"thema\":config['target_dir'],\n",
    "        \"exp_id\":exp_id,\n",
    "        \"model_id\":train_id,\n",
    "        \"use_mask\":use_mask,\n",
    "        # \"mask_power\":mask_power,\n",
    "        # \"mask_alpha\":mask_alpha,\n",
    "        \"epoch\":train_dataset.__len__(),\n",
    "    })\n",
    "# 16c91bc3767f14563f500e1342430f71c8b26518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "554d5423-9d01-4fd4-948b-20bd0c795de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "    initial_global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ebf1697-5c60-4eca-88f6-b0012fce7b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "    progress_bar = tqdm(\n",
    "        range(0, max_train_steps),\n",
    "        initial=initial_global_step,\n",
    "        desc=\"Steps\",\n",
    "        # Only show the progress bar once on each machine.\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "    \n",
    "    best_loss = 1000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6280158-bbe6-4100-a278-bea15957fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "\n",
    "# ds = train_dataset[random.randint(0, 100)]\n",
    "\n",
    "# f, axarr = plt.subplots(2 * len(ds['pixel_values']), 3, figsize=(10, 20))\n",
    "\n",
    "# for img_num in range(len(ds['pixel_values'])):\n",
    "#     for n in range(3):\n",
    "#         axarr[img_num * 2][n].imshow(ds['pixel_values'][img_num][n])\n",
    "#     for n in range(3):\n",
    "#         axarr[img_num * 2 + 1][n].imshow(ds['mask_pixel_values'][img_num][n])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01514850-3cd6-437c-bf53-9e428bfb01af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/oa_venv/lib/python3.10/site-packages/diffusers/models/lora.py:228: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(\n",
      "/workspace/oa_venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "Steps:   0%|          | 1/200 [00:13<44:31, 13.42s/it]04/29/2025 20:18:14 - INFO - __main__ - Saving embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss:0.1687372624874115 @@@ Step:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   2%|▏         | 3/200 [00:23<22:14,  6.77s/it, loss=0.215, lr=0.0001]04/29/2025 20:18:24 - INFO - __main__ - Saving embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss:0.14465458691120148 @@@ Step:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   4%|▍         | 9/200 [00:50<15:04,  4.74s/it, loss=0.344, lr=0.0001]04/29/2025 20:18:52 - INFO - __main__ - Saving embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss:0.11338549852371216 @@@ Step:9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   5%|▌         | 10/200 [00:55<15:13,  4.81s/it, loss=0.113, lr=0.0001]04/29/2025 20:18:57 - INFO - __main__ - Saving embeddings\n",
      "Steps:  10%|▉         | 19/200 [01:39<13:52,  4.60s/it, loss=0.124, lr=0.0001]04/29/2025 20:19:40 - INFO - __main__ - Saving embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss:0.10834071040153503 @@@ Step:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  10%|█         | 20/200 [01:43<14:04,  4.69s/it, loss=0.108, lr=0.0001]04/29/2025 20:19:45 - INFO - __main__ - Saving embeddings\n",
      "Steps:  15%|█▌        | 30/200 [02:30<13:00,  4.59s/it, loss=0.198, lr=0.0001]04/29/2025 20:20:31 - INFO - __main__ - Saving embeddings\n",
      "Steps:  18%|█▊        | 37/200 [03:02<12:29,  4.60s/it, loss=0.355, lr=0.0001]04/29/2025 20:21:04 - INFO - __main__ - Saving embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss:0.06241400167346001 @@@ Step:37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  20%|██        | 40/200 [03:17<12:38,  4.74s/it, loss=0.383, lr=0.0001] 04/29/2025 20:21:18 - INFO - __main__ - Saving embeddings\n",
      "Steps:  25%|██▌       | 50/200 [04:03<11:42,  4.69s/it, loss=0.342, lr=0.0001]04/29/2025 20:22:04 - INFO - __main__ - Saving embeddings\n",
      "Steps:  30%|███       | 60/200 [04:49<10:53,  4.67s/it, loss=0.39, lr=0.0001] 04/29/2025 20:22:50 - INFO - __main__ - Saving embeddings\n",
      "Steps:  35%|███▌      | 70/200 [05:35<09:58,  4.60s/it, loss=0.367, lr=0.0001]04/29/2025 20:23:36 - INFO - __main__ - Saving embeddings\n",
      "Steps:  40%|████      | 80/200 [06:20<09:04,  4.54s/it, loss=0.25, lr=0.0001] 04/29/2025 20:24:22 - INFO - __main__ - Saving embeddings\n",
      "Steps:  45%|████▌     | 90/200 [07:06<08:18,  4.53s/it, loss=0.118, lr=0.0001]04/29/2025 20:25:07 - INFO - __main__ - Saving embeddings\n",
      "Steps:  50%|█████     | 100/200 [07:52<07:40,  4.60s/it, loss=0.299, lr=0.0001]04/29/2025 20:25:53 - INFO - __main__ - Saving embeddings\n",
      "Steps:  55%|█████▌    | 110/200 [08:37<06:47,  4.52s/it, loss=0.263, lr=0.0001]04/29/2025 20:26:39 - INFO - __main__ - Saving embeddings\n",
      "Steps:  60%|██████    | 120/200 [09:23<06:06,  4.58s/it, loss=0.203, lr=0.0001]04/29/2025 20:27:24 - INFO - __main__ - Saving embeddings\n",
      "Steps:  65%|██████▌   | 130/200 [10:09<05:17,  4.54s/it, loss=0.187, lr=0.0001]04/29/2025 20:28:10 - INFO - __main__ - Saving embeddings\n",
      "Steps:  70%|███████   | 140/200 [10:55<04:34,  4.57s/it, loss=0.386, lr=0.0001]04/29/2025 20:28:56 - INFO - __main__ - Saving embeddings\n",
      "Steps:  75%|███████▌  | 150/200 [11:41<03:48,  4.58s/it, loss=0.324, lr=0.0001] 04/29/2025 20:29:42 - INFO - __main__ - Saving embeddings\n",
      "Steps:  80%|████████  | 160/200 [12:27<03:02,  4.55s/it, loss=0.113, lr=0.0001] 04/29/2025 20:30:28 - INFO - __main__ - Saving embeddings\n",
      "Steps:  85%|████████▌ | 170/200 [13:12<02:16,  4.54s/it, loss=0.228, lr=0.0001]04/29/2025 20:31:14 - INFO - __main__ - Saving embeddings\n",
      "Steps:  90%|█████████ | 180/200 [13:58<01:30,  4.52s/it, loss=0.351, lr=0.0001]04/29/2025 20:31:59 - INFO - __main__ - Saving embeddings\n",
      "Steps:  95%|█████████▌| 190/200 [14:43<00:45,  4.52s/it, loss=0.296, lr=0.0001]04/29/2025 20:32:45 - INFO - __main__ - Saving embeddings\n",
      "Steps: 100%|██████████| 200/200 [15:29<00:00,  4.47s/it, loss=0.105, lr=0.0001]04/29/2025 20:33:30 - INFO - __main__ - Saving embeddings\n",
      "Steps: 100%|██████████| 200/200 [15:29<00:00,  4.47s/it, loss=0.415, lr=0.0001]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss:0.06241400167346001 @@@ Step:37\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▇▂▂▇▄▂▃▃▁█▂▆▂▅▇▇▅▅▄▃▄▂▅▅▆▃▂▂▃▅▇▃▅▆▆▅▄▆▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.41545</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-dawn-10</strong> at: <a href='https://wandb.ai/eadishcheva/OneActor/runs/y2dbtabw' target=\"_blank\">https://wandb.ai/eadishcheva/OneActor/runs/y2dbtabw</a><br> View project at: <a href='https://wandb.ai/eadishcheva/OneActor' target=\"_blank\">https://wandb.ai/eadishcheva/OneActor</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250429_201800-y2dbtabw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    mask_power = 0.5\n",
    "    best_loss = 1000.0\n",
    "    vae = vae.to(dtype=torch.float32)\n",
    "            \n",
    "    for epoch in range(first_epoch, config['epochs']):\n",
    "        projector.train()\n",
    "        train_loss = 0.0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(projector):\n",
    "   \n",
    "                images = batch[\"pixel_values\"][0].to(dtype=torch.float32)\n",
    "                latents = vae.encode(images).latent_dist.sample().detach()                    \n",
    "                latents.to(dtype=weight_dtype)\n",
    "                latents = latents * vae.config.scaling_factor\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                noise[-1] = noise[0]    # aver is the same as target\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "                timesteps = timesteps.long()\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                if use_mask:            \n",
    "                    mask_latents = batch[\"mask_pixel_values\"][0].to(dtype=weight_dtype)\n",
    "\n",
    "                    noisy_mask_latents = ((1 - noise_scheduler.alphas_cumprod[timesteps.cpu()])**mask_power).view(bsz, 1, 1, 1).to(device)\n",
    "                    noisy_mask_latents = noisy_mask_latents.to(device)\n",
    "                    noisy_mask_latents = noisy_mask_latents \\\n",
    "                            + ((noise_scheduler.alphas_cumprod[timesteps.cpu()])**mask_power).view(bsz, 1, 1, 1).to(device) * mask_latents\n",
    "    \n",
    "                # time ids\n",
    "                def compute_time_ids(original_size, crops_coords_top_left):\n",
    "                    # Adapted from pipeline.StableDiffusionXLPipeline._get_add_time_ids\n",
    "                    target_size = (1024, 1024)\n",
    "                    add_time_ids = list(original_size + crops_coords_top_left + target_size)\n",
    "                    add_time_ids = torch.tensor([add_time_ids])\n",
    "                    with torch.cuda.amp.autocast(dtype=weight_dtype):\n",
    "                        add_time_ids = add_time_ids.to(accelerator.device)\n",
    "                    return add_time_ids # tensor(1, 6)\n",
    "                \n",
    "                original_size = (1024, 1024)\n",
    "                crops_coords_top_left = (0, 0)\n",
    "                add_time_ids = compute_time_ids(original_size, crops_coords_top_left).repeat(bsz, 1).to(accelerator.device)\n",
    "                unet_added_conditions = {\"time_ids\": add_time_ids}\n",
    "    \n",
    "                text_encoders = [text_encoder_one, text_encoder_two]\n",
    "                tokenizers = [tokenizer_one, tokenizer_two]\n",
    "                # Get the text embedding for conditioning\n",
    "                prompt_embeds_batch_list = []\n",
    "                add_text_embeds_batch_list = []\n",
    "                delta_emb = projector(batch['h_mid'][0, :, -1].to(device)) # torch.size(bs, 1280, 32, 32) -> torch.size(bs, 2048)\n",
    "                delta_emb_aver = delta_emb[1:-1].mean(dim=0, keepdim=True)\n",
    "                for b_s in range(bsz): # 1*target+n*base+1*aver\n",
    "                    prompt = batch['text'][b_s][0] # str\n",
    "                    prompt_embeds_list = []\n",
    "    \n",
    "                    first = 1\n",
    "                    for tokenizer, text_encoder in zip(tokenizers, text_encoders):\n",
    "    \n",
    "                        text_inputs = tokenizer(\n",
    "                            prompt,\n",
    "                            padding=\"max_length\",\n",
    "                            max_length=tokenizer.model_max_length,\n",
    "                            truncation=True,\n",
    "                            return_tensors=\"pt\",\n",
    "                        )\n",
    "                        tokens = tokenizer.encode(prompt)\n",
    "                        \n",
    "                        if first:\n",
    "                            for i, token in enumerate(tokens):\n",
    "                                if tokenizer.decode(token) == batch['base'][0]:\n",
    "                                    base_token_id = i\n",
    "                                    first = 0\n",
    "                                    break\n",
    "                        text_input_ids = text_inputs.input_ids\n",
    "                        prompt_embeds = text_encoder(text_input_ids.to(device), output_hidden_states=True)\n",
    "                        pooled_prompt_embeds = prompt_embeds[0]\n",
    "                        prompt_embeds = prompt_embeds.hidden_states[-2]\n",
    "                        prompt_embeds_list.append(prompt_embeds)\n",
    "                    prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)    # tensor(1, 77, 2048)\n",
    "    \n",
    "                    if b_s == bsz-1:\n",
    "                        delta_emb_ = delta_emb_aver\n",
    "                    else:\n",
    "                        delta_emb_ = delta_emb[b_s:b_s+1]\n",
    "                    prompt_embeds[:, base_token_id, :] = prompt_embeds[:, base_token_id, :] + delta_emb_\n",
    "\n",
    "                    prompt_embeds_batch_list.append(prompt_embeds)\n",
    "                    add_text_embeds_batch_list.append(pooled_prompt_embeds)\n",
    "                \n",
    "                prompt_embeds = torch.concat(prompt_embeds_batch_list, dim=0)\n",
    "                add_text_embeds = torch.concat(add_text_embeds_batch_list, dim=0).to(accelerator.device)\n",
    "    \n",
    "                unet_added_conditions.update({\"text_embeds\": add_text_embeds})\n",
    "                prompt_embeds = prompt_embeds.to(accelerator.device)\n",
    "    \n",
    "    \n",
    "                # Predict the noise residual\n",
    "                dtype = unet.dtype\n",
    "\n",
    "                noisy_latents = noisy_latents.to(dtype)\n",
    "                prompt_embeds = prompt_embeds.to(dtype)\n",
    "                timesteps = timesteps.to(dtype)\n",
    "                unet_added_conditions[\"text_embeds\"] = unet_added_conditions[\"text_embeds\"].to(dtype)\n",
    "                unet_added_conditions[\"time_ids\"] = unet_added_conditions[\"time_ids\"].to(dtype)\n",
    "\n",
    "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states=prompt_embeds, added_cond_kwargs=unet_added_conditions).sample\n",
    "    \n",
    "    \n",
    "                # Get the target for loss depending on the prediction type\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "                if use_mask:\n",
    "                    loss_target = F.mse_loss(model_pred[:1].float() * noisy_mask_latents[:1].float(),\n",
    "                                             target[:1].float() * noisy_mask_latents[:1].float(), reduction=\"mean\")\n",
    "                    loss_base = F.mse_loss(model_pred[1:-1].float() * noisy_mask_latents[1:-1].float(),\n",
    "                                             target[1:-1].float() * noisy_mask_latents[1:-1].float(), reduction=\"mean\")\n",
    "                    loss_aver = F.mse_loss(model_pred[-1:].float() * noisy_mask_latents[-1:].float(),\n",
    "                                             target[-1:].float() * noisy_mask_latents[-1:].float(), reduction=\"mean\")\n",
    "                else:\n",
    "                    loss_target = F.mse_loss(model_pred[:1].float(),\n",
    "                                             target[:1].float(), reduction=\"mean\")\n",
    "                    loss_base = F.mse_loss(model_pred[1:-1].float(),\n",
    "                                             target[1:-1].float(), reduction=\"mean\")\n",
    "                    loss_aver = F.mse_loss(model_pred[-1:].float(),\n",
    "                                             target[-1:].float(), reduction=\"mean\")\n",
    "                    \n",
    "                loss = loss_target + config['lambda1'] * loss_base + config['lambda2'] * loss_aver    \n",
    "                avg_loss = accelerator.gather(loss.repeat(config['batch_size'])).mean()\n",
    "                train_loss += avg_loss.item()\n",
    "\n",
    "                run.log({\"loss\": loss})\n",
    "\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "    \n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                if global_step % config['save_steps'] == 0:\n",
    "                    weight_name = (\n",
    "                        f\"learned-projector-steps-{global_step}.pth\"\n",
    "                    )\n",
    "                    save_path = os.path.join(output_dir, 'weight', weight_name)\n",
    "\n",
    "                    save_progress(\n",
    "                        projector,\n",
    "                        accelerator,\n",
    "                        save_path,\n",
    "                    )\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    best_step = initial_global_step + global_step\n",
    "                    print(f'Best loss:{best_loss} @@@ Step:{best_step}')\n",
    "                    weight_name = (\n",
    "                        f\"best-learned-projector.pth\"\n",
    "                    )\n",
    "                    save_path = os.path.join(output_dir, 'weight', weight_name)\n",
    "\n",
    "                    save_progress(\n",
    "                        projector,\n",
    "                        accelerator,\n",
    "                        save_path,\n",
    "                    )\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    if global_step % config['checkpointing_steps'] == 0:\n",
    "                        save_path = os.path.join(output_dir, 'ckpt', f\"checkpoint-{initial_global_step + global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "                        logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "            logs = {\"loss\": train_loss, \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            train_loss = 0.0\n",
    "\n",
    "    # Create the pipeline using the trained modules and save it.\n",
    "    accelerator.wait_for_everyone()\n",
    "   \n",
    "    print(f'Best loss:{best_loss} @@@ Step:{best_step}')\n",
    "    accelerator.end_training()\n",
    "    # Finish the run and upload any remaining data.\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90335ed-a93d-43b0-a8be-e1e454cdacba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (oa_venv)",
   "language": "python",
   "name": "oa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
