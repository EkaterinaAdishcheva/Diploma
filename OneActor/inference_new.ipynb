{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc8f4a3-8f1d-4030-905d-9b6ca12514f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/oa_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/oa_venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/workspace/oa_venv/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "import yaml\n",
    "import argparse\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bffa69e-446a-4b4d-b412-fe272c7c0bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/oa_venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/workspace/oa_venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/workspace/oa_venv/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from pipeline_stable_diffusion_xl import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7884c5d9-ab6e-46b5-bd4b-0523d22621b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def find_token_ids(tokenizer, prompt, words):\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    ids = []\n",
    "    if isinstance(words, str):\n",
    "                  words = [words]\n",
    "    for word in words:\n",
    "        for i, token in enumerate(tokens):\n",
    "            if tokenizer.decode(token) == word:\n",
    "                ids.append(i)\n",
    "                break\n",
    "    assert len(ids) != 0 , 'Cannot find the word in the prompt.'\n",
    "    return ids\n",
    "\n",
    "def projector_inference(projector_path, h_target, h_base, device):\n",
    "    with torch.no_grad():\n",
    "        projector = torch.load(projector_path).to(device)\n",
    "        mid_base_target = h_base + [h_target[-1]]\n",
    "        mid_base_all = torch.stack(mid_base_target)\n",
    "        projector = projector.half()\n",
    "        mid_base_all=mid_base_all.half()\n",
    "        delta_emb_all = projector(mid_base_all[:,-1].to(device))\n",
    "\n",
    "    return delta_emb_all\n",
    "\n",
    "def pipeline_inference(pipeline, prompt, neg_prompt, config, oneactor_extra_config, generator=None):\n",
    "    if generator is None:\n",
    "        generator = torch.manual_seed(config['seed'])\n",
    "    return pipeline(\n",
    "            prompt,\n",
    "            negative_prompt=neg_prompt,\n",
    "            num_inference_steps=config['inference_steps'], guidance_scale=config['eta_1'], \\\n",
    "            generator=generator, oneactor_extra_config=oneactor_extra_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b65be8-69b2-4cb6-8565-acf6d2200e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(\"PATH.json\",\"r\") as f:\n",
    "        ENV_CONFIGS = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92dd452-a6ff-4a63-9dea-0f518eb44b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = \"2504292003\"\n",
    "model_id = \"output_2504292017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a5c6bb2-2512-40c8-b678-e3d574e00653",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(\"./config/gen_cs_adventure.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b3938b-7d7d-49b8-b708-ee9f4cf63896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_id = 2504292003\n",
      "model_id = output_2504292017\n",
      "Save inference in experiments/consistory_adventurer/2504292003/output_2504292017/inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  8.58it/s]\n"
     ]
    }
   ],
   "source": [
    "    tgt_dirs = []\n",
    "    target_dir = config['experiments_dir']+'/'+config['target_dir']\n",
    "    for _, tgt_dirs, _ in os.walk(target_dir):\n",
    "        break\n",
    "        \n",
    "\n",
    "    print(f\"target_id = {target_id}\")\n",
    "\n",
    "\n",
    "    if target_id not in tgt_dirs:\n",
    "        print(\"Base image is not generated\")\n",
    "\n",
    "    target_dir += f\"/{target_id}\"\n",
    "\n",
    "    print(f\"model_id = {model_id}\")\n",
    "    \n",
    "    for _, tgt_dirs, _ in os.walk(target_dir):\n",
    "        break\n",
    "    \n",
    "    if model_id not in tgt_dirs:\n",
    "        print(\"Train is not performed\")\n",
    "\n",
    "    \n",
    "    out_root = target_dir + f\"/{model_id}\" \n",
    "    \n",
    "    os.makedirs(f\"{out_root}/inference\", exist_ok=True)\n",
    "    print(f\"Save inference in {out_root}/inference\")\n",
    "\n",
    "\n",
    "    # load sd pipeline\n",
    "    pipeline = DiffusionPipeline.from_pretrained(ENV_CONFIGS['paths']['sdxl_path']).to(config['device'])\n",
    "    pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0a20d6-c089-4a53-99f4-2bc82283e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(target_dir+f'/target_data.pkl', 'rb') as f:\n",
    "        target_data = pickle.load(f)\n",
    "\n",
    "    with open(target_dir+f'/base/base_data_list.pkl', 'rb') as f:\n",
    "        base_data = pickle.load(f)\n",
    "\n",
    "\n",
    "    h_base = [h['h_mid'][-1] for h in base_data]\n",
    "    h_tar = target_data['h_mid']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "605f5c23-3e64-4fed-ace7-4dd91901ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    config['neg_prompts'] = [''] * len(config['add_prompts'])\n",
    "    config['file_names'] = [\"_\".join(prompt.split(\" \")) for prompt in config['add_prompts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "119daf6c-1fec-43f9-ae08-ef2a76484a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompt A rugger adventurer with tousled hair, comic book stile  a city as background...\n",
      "Using weights from step 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/oa_venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 30/30 [00:23<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "    # iterate over image list\n",
    "    for img_num in range(len(config['add_prompts'])):\n",
    "        _str = config['target_prompt'] + \" \" + config['add_target_prompt'] + \" \" + config['add_prompts'][img_num]\n",
    "        print(f\"Generating prompt {_str}...\")\n",
    "        # original output by SDXL\n",
    "        generator = torch.manual_seed(config['seed'])\n",
    "\n",
    "        # perform step-wise guidance\n",
    "        select_steps = config['select_steps']\n",
    "        if select_steps is not False:\n",
    "            assert (len(select_steps) % 2) == 0\n",
    "            select_list = []\n",
    "            for _ in range(len(select_steps) // 2):\n",
    "                a = select_steps[2*_]\n",
    "                b = select_steps[2*_ + 1]\n",
    "                select_list = select_list + list(range(a-1,b))\n",
    "        else:\n",
    "            select_list = None\n",
    "\n",
    "        # locate the base token id\n",
    "        token_id = find_token_ids(pipeline.tokenizer, config['target_prompt'] + \" \" + config['add_prompts'][img_num], config['base'])\n",
    "        generator = torch.manual_seed(config['seed'])\n",
    "        config['generator'] = generator\n",
    "\n",
    "        if config['only_step'] is False:\n",
    "            for i in range(50):\n",
    "                steps = config['step_from']+config['step']*(i)\n",
    "                print(f\"Using weights from step (steps)\")\n",
    "                with torch.no_grad():\n",
    "                    projector_path = f'{out_root}/weight/learned-projector-steps-{steps}.pth'\n",
    "                    delta_emb_all = projector_inference(projector_path, h_tar, h_base, config['device']).to(config['device'])\n",
    "                    print(delta_emb_all.mean())\n",
    "\n",
    "                delta_emb_aver = delta_emb_all[:-1].mean(dim=0)\n",
    "                delta_emb_tar = config['v'] * delta_emb_all[-1]\n",
    "\n",
    "                oneactor_extra_config = {\n",
    "                    'token_ids': token_id,\n",
    "                    'delta_embs': delta_emb_tar,\n",
    "                    'delta_steps': select_list,\n",
    "                    'eta_2': config['eta_2'],\n",
    "                    'delta_emb_aver': delta_emb_aver\n",
    "                }\n",
    "\n",
    "                image = pipeline_inference(\n",
    "                    pipeline, \n",
    "                    config['target_prompt'] + \" \" + config['add_target_prompt'] + \" \" + config['add_prompts'][img_num],\n",
    "                    config['target_neg_prompt'] + \" \" + config['neg_prompts'][img_num],\n",
    "                    config, oneactor_extra_config)\n",
    "                image = image.images[0]\n",
    "                image.save(f\"{out_root}/inference/{config['file_names'][img_num]}_step_{steps}.jpg\")\n",
    "        elif config['only_step'] == 'best':\n",
    "            with torch.no_grad():\n",
    "                projector_path = f'{out_root}/weight/best-learned-projector.pth'\n",
    "                delta_emb_all = projector_inference(projector_path, h_tar, h_base, config['device']).to(config['device'])\n",
    "\n",
    "            delta_emb_aver = delta_emb_all[:-1].mean(dim=0) # [2048]\n",
    "            delta_emb_tar = config['v'] * delta_emb_all[-1] # [2048]\n",
    "            \n",
    "            oneactor_extra_config = {\n",
    "                'token_ids': token_id,\n",
    "                'delta_embs': delta_emb_tar,\n",
    "                'delta_steps': select_list,\n",
    "                'eta_2': config['eta_2'],\n",
    "                'delta_emb_aver': delta_emb_aver\n",
    "            }\n",
    "            image = pipeline_inference(\n",
    "                pipeline,\n",
    "                config['target_prompt'] + \" \" + config['add_target_prompt'] + \" \" + config['add_prompts'][img_num],\n",
    "                config['target_neg_prompt'] + \" \" + config['neg_prompts'][img_num],\n",
    "                config, oneactor_extra_config)\n",
    "            image = image.images[0]\n",
    "            image.save(f\"{out_root}/inference/{config['file_names'][img_num]}_step_best.jpg\")\n",
    "        else:\n",
    "            steps_list = config['only_step']\n",
    "            for steps in steps_list:\n",
    "                print(f\"Using weights from step {steps}\")\n",
    "                with torch.no_grad():\n",
    "                    projector_path = f'{out_root}/weight/learned-projector-steps-{steps}.pth'\n",
    "                    delta_emb_all = projector_inference(projector_path, h_tar, h_base, config['device']).to(config['device'])\n",
    "\n",
    "                delta_emb_aver = delta_emb_all[:-1].mean(dim=0) # [2048]\n",
    "                delta_emb_tar = config['v'] * delta_emb_all[-1] # [2048]\n",
    "\n",
    "                oneactor_extra_config = {\n",
    "                    'token_ids': token_id,\n",
    "                    'delta_embs': delta_emb_tar,\n",
    "                    'delta_steps': select_list,\n",
    "                    'eta_2': config['eta_2'],\n",
    "                    'delta_emb_aver': delta_emb_aver\n",
    "                }\n",
    "                image = pipeline_inference(\n",
    "                    pipeline, \n",
    "                    config['target_prompt'] + \" \" + config['add_target_prompt'] + \" \" + config['add_prompts'][img_num],\n",
    "                    config['target_neg_prompt'] + \" \" + config['neg_prompts'][img_num],\n",
    "                    config, oneactor_extra_config)\n",
    "                image = image.images[0]\n",
    "                image.save(f\"{out_root}/inference/{config['file_names'][img_num]}_step_{str(steps)}.jpg\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a5690-804f-4523-8914-a8afcc1d0611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (oa_venv)",
   "language": "python",
   "name": "oa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
