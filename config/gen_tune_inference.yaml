
# GENERAL SETTING
data_root: 'data/demo'
output_dir: 'output/demo'
out_file: '/inference'
pretrain_root: 'output/demo'

model_type: 'XL'
size: 1024
device: 'cuda'

# GENERATE
source_prompt: 'man with a mustach and a hat, fauvism' # CHANGE HERE, MAKE SURE IT CONTAINS THE BASE WORD
base_condition: 'man' # CHANGE HERE
prompt: 'man with a mustach and a hat, fauvism'
base: 'man'
source_neg_prompt: ''
add_prompts: ['in the park', 'reading a book', 'at the beach', 'holding an avacado']
neg_prompts: ['', '', '', '']
file_names: ['a_man_with_a_mustach_and_a_hat_fauvism_in_the_park', 'a_man_with_a_mustach_and_a_hat_fauvism_reading_a_book',      
    'a_man_with_a_mustach_and_a_hat_fauvism_at_the_beach', 'a_man_with_a_mustach_and_a_hat_fauvism_holding_an_avacado']


steps: 30
guidance_scale: 7.5
g_seed: 6543
gen_base: 10 # Generate auxiliary images

# TUNE
concept_type: 'character'  # CHANGE HERE, choose from 'character' 'object' 'style'
t_seed: 999
lr: 0.0001
neg_num: 3 # Auxiliary images used in a batch
lambda1: 0.5
lambda2: 0.2
batch_size: 1 # Fixed
epochs: 1
save_steps: 10
checkpointing_steps: 500
resume_from_checkpoint: None  # True or None
xt_to_use: -1

# env config
use_xformers: True
allow_tf32: True


# INFERENCE

seed: 23018
inference_steps: 30
select_steps: [1,20]  # appplying cluster guidance to these steps
only_step: [20, 50, 70, 120, 150, 170]  # type in a specific step (Recommended) or set to 'best'

v: 0.8 # semantic interpolation scales
eta_1: 7.5  # target attraction scale
eta_2: 0  # auxiliary exclusion scale (set to 0 will avoid increasing inference time)

xt_position: -1
